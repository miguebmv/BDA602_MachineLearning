{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "import pandas as pd\n",
    "import geopandas\n",
    "import matplotlib.pyplot as plt\n",
    "from geodatasets import get_path\n",
    "\n",
    "## Download data##\n",
    "flight_data = pd.read_csv(\"/mnt/c/Users/tbrag/OneDrive/Desktop/bda602venv/ML Project/flights_sample_3m.csv\")\n",
    "airport_data = pd.read_csv(\"/mnt/c/Users/tbrag/OneDrive/Desktop/bda602venv/ML Project/iata-icao.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##How many airports in the flight data##\n",
    "print(len(flight_data['ORIGIN'].unique()))\n",
    "print(len(flight_data['DEST'].unique()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###remove uneeded columns in airport data##\n",
    "airport_data.drop(columns=airport_data.columns.difference(['iata', 'latitude', 'longitude']), inplace=True)\n",
    "airport_data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge flight_data with airport_data for origin airport\n",
    "flight_data = flight_data.merge(airport_data, left_on='ORIGIN', right_on='iata', how='left')\n",
    "\n",
    "# Rename columns to indicate origin airport latitude and longitude\n",
    "flight_data = flight_data.rename(columns={'latitude': 'origin_latitude', 'longitude': 'origin_longitude'})\n",
    "\n",
    "# Merge flight_data with airport_data for destination airport\n",
    "flight_data = flight_data.merge(airport_data, left_on='DEST', right_on='iata', how='left')\n",
    "\n",
    "# Rename columns to indicate destination airport latitude and longitude\n",
    "flight_data = flight_data.rename(columns={'latitude': 'dest_latitude', 'longitude': 'dest_longitude'})\n",
    "\n",
    "# Drop unnecessary columns (iata_x and iata_y)\n",
    "flight_data.drop(columns=['iata_x', 'iata_y'], inplace=True)\n",
    "\n",
    "flight_data.head(5)\n",
    "\n",
    "##export the new table##\n",
    "flight_data.to_csv('flight_data_with_coordinates.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flight_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Path to station txt file\n",
    "station_file = '/mnt/c/Users/tbrag/OneDrive/Desktop/bda602venv/ghcnd-stations (2).txt'\n",
    "\n",
    "# Define column widths for fixed-width parsing\n",
    "col_widths = [12, 9, 10, 6, 50]  # Adjust the widths based on your data\n",
    "\n",
    "# Create a DataFrame from the data\n",
    "station_df = pd.read_fwf(station_file, widths=col_widths, header=None, names=['Station_ID', 'Latitude', 'Longitude', 'Elevation', 'Station_Name'])\n",
    "\n",
    "# Display the first 5 rows of the DataFrame\n",
    "print(station_df.head())\n",
    "\n",
    "#drop not relevent columns\n",
    "station_df.drop(columns=['Elevation', 'Station_Name'], inplace=True)\n",
    "                         \n",
    "print(station_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely.geometry import Point, LineString\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "from geodatasets import get_path\n",
    "\n",
    "\n",
    "\n",
    "# Create Point geometries for origin and destination\n",
    "#flight_data['origin_geometry'] = flight_data.apply(lambda row: Point(row['origin_longitude'], row['origin_latitude']), axis=1)\n",
    "#flight_data['dest_geometry'] = flight_data.apply(lambda row: Point(row['dest_longitude'], row['dest_latitude']), axis=1)\n",
    "\n",
    "# Create LineString geometries\n",
    "#flight_data['geometry'] = flight_data.apply(lambda row: LineString([row['origin_geometry'], row['dest_geometry']]), axis=1)\n",
    "\n",
    "# Create GeoDataFrame\n",
    "#flight_lines_gdf = gpd.GeoDataFrame(flight_data, geometry='geometry')\n",
    "\n",
    "##export the new table##\n",
    "#flight_lines_gdf.to_csv('flightpath_lines.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Make station Lat Long into point geometry and create a geopandas dataframe\n",
    "station_gdf = geopandas.GeoDataFrame(\n",
    "    station_df, geometry=geopandas.points_from_xy(station_df.Longitude, station_df.Latitude), crs=\"EPSG:4326\"\n",
    ")\n",
    "station_gdf.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##make flight into geo df using origin Lat Long\n",
    "\n",
    "flight_gdf = geopandas.GeoDataFrame(\n",
    "    flight_data, geometry=geopandas.points_from_xy(flight_data.origin_longitude, flight_data.origin_latitude), crs=\"EPSG:4326\"\n",
    ")\n",
    "flight_gdf.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_gdf = flight_gdf.sjoin_nearest(station_gdf,how=\"inner\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing created right index column\n",
    "combined_gdf = combined_gdf.drop(columns = \"index_right\")\n",
    "\n",
    "#renaming some columns\n",
    "combined_gdf = combined_gdf.rename(columns = {'geometry':'origin_geometry','Station_ID':'origin_station_id','Latitude':'ori_station_lat','Longitude':'ori_station_long'})\n",
    "\n",
    "combined_gdf.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique values in the origin station id column\n",
    "station_values = combined_gdf['origin_station_id'].unique()\n",
    "\n",
    "# create a list of unique stations to use as a filter later\n",
    "station_list = list(station_values)\n",
    "\n",
    "print(station_list)\n",
    "print(len(station_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a dictionary to map origin coordinates to station IDs\n",
    "origin_coords_to_id = dict(zip(zip(combined_gdf['origin_latitude'], combined_gdf['origin_longitude']), combined_gdf['origin_station_id']))\n",
    "\n",
    "# Create a new column 'dest_station_id' based on matching dest coordinates\n",
    "combined_gdf['dest_station_id'] = list(map(lambda coord: origin_coords_to_id.get(coord), zip(combined_gdf['dest_latitude'], combined_gdf['dest_longitude'])))\n",
    "\n",
    "\n",
    "combined_gdf.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(combined_gdf['dest_station_id'].unique()))\n",
    "print(len(combined_gdf['origin_station_id'].unique()))\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "df =pd.read_parquet(\"/mnt/c/Users/tbrag/OneDrive/Desktop/bda602venv/5a6b3c9b29514ca58a2f53f8e4521d65_1.snappy.parquet\")\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "def process_parquet_file(parquet_file, prev_file, station_list, new_column_names):\n",
    "   \n",
    "\n",
    "    # Read the Parquet file into a Pandas DataFrame\n",
    "    df = pd.read_parquet(parquet_file)\n",
    "\n",
    "    # Rename columns\n",
    "    df = df.rename(columns=new_column_names)\n",
    "\n",
    "    df = df.drop(columns=[\"M_FLAG\",\"Q_FLAG\",\"S_FLAG\",\"OBS_TIME\"])\n",
    "\n",
    "    # Filter rows based on ID list\n",
    "    filtered_df = df[df['station_id'].isin(station_list)]\n",
    "\n",
    "   # Concatenate with previous DataFrame if it exists\n",
    "    if prev_file is not None:\n",
    "        concat_df = pd.concat([prev_file, filtered_df], ignore_index=True)\n",
    "    else:\n",
    "        concat_df = filtered_df\n",
    "\n",
    "    return concat_df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Define processed_dfs as an empty list before the loop\n",
    "processed_prcp_dfs = []\n",
    "\n",
    "# Iterate through files in the folder\n",
    "folder_path = \"/mnt/c/Users/tbrag/OneDrive/Desktop/bda602venv/ML Project/weather parquet files/PRCP\"\n",
    "for file_name in os.listdir(folder_path):\n",
    "    if file_name.endswith(\".parquet\"):\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        print(f\"Processing file: {file_path}\")\n",
    "        # Process each file and append the result to processed_dfs\n",
    "        result_df = process_parquet_file(file_path, None, station_list, {\"ID\":\"station_id\",\"DATA_VALUE\":\"PCRP\"})\n",
    "        processed_prcp_dfs.append(result_df)\n",
    "        print(f\"Number of rows in result_df: {len(result_df)}\")\n",
    "\n",
    "# Concatenate all processed DataFrames into a single DataFrame\n",
    "prcp_df = pd.concat(processed_prcp_dfs, ignore_index=True)\n",
    "\n",
    "# Display the first 10 rows of the concatenated DataFrame\n",
    "print(prcp_df.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define processed_dfs for AWND as an empty list before the loop\n",
    "processed_awnd_dfs = []\n",
    "\n",
    "# Iterate through files in the folder\n",
    "folder_path = \"/mnt/c/Users/tbrag/OneDrive/Desktop/bda602venv/ML Project/weather parquet files/AWND\"\n",
    "for file_name in os.listdir(folder_path):\n",
    "    if file_name.endswith(\".parquet\"):\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        print(f\"Processing file: {file_path}\")\n",
    "        # Process each file and append the result to processed_dfs\n",
    "        result_df = process_parquet_file(file_path, None, station_list, {\"ID\":\"station_id\",\"DATA_VALUE\":\"AWND\"})\n",
    "        processed_awnd_dfs.append(result_df)\n",
    "        print(f\"Number of rows in result_df: {len(result_df)}\")\n",
    "\n",
    "# Concatenate all processed DataFrames into a single DataFrame\n",
    "awnd_df = pd.concat(processed_awnd_dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define processed_dfs for snow as an empty list before the loop\n",
    "processed_snow_dfs = []\n",
    "\n",
    "# Iterate through files in the folder\n",
    "folder_path = \"/mnt/c/Users/tbrag/OneDrive/Desktop/bda602venv/ML Project/weather parquet files/SNOW\"\n",
    "for file_name in os.listdir(folder_path):\n",
    "    if file_name.endswith(\".parquet\"):\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        print(f\"Processing file: {file_path}\")\n",
    "        # Process each file and append the result to processed_dfs\n",
    "        result_df = process_parquet_file(file_path, None, station_list, {\"ID\":\"station_id\",\"DATA_VALUE\":\"SNOW\"})\n",
    "        processed_snow_dfs.append(result_df)\n",
    "        print(f\"Number of rows in result_df: {len(result_df)}\")\n",
    "\n",
    "# Concatenate all processed DataFrames into a single DataFrame\n",
    "snow_df = pd.concat(processed_snow_dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define processed_dfs for snow depth as an empty list before the loop\n",
    "processed_snwd_dfs = []\n",
    "\n",
    "# Iterate through files in the folder\n",
    "folder_path = \"/mnt/c/Users/tbrag/OneDrive/Desktop/bda602venv/ML Project/weather parquet files/SNWD\"\n",
    "for file_name in os.listdir(folder_path):\n",
    "    if file_name.endswith(\".parquet\"):\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        print(f\"Processing file: {file_path}\")\n",
    "        # Process each file and append the result to processed_dfs\n",
    "        result_df = process_parquet_file(file_path, None, station_list, {\"ID\":\"station_id\",\"DATA_VALUE\":\"SNWD\"})\n",
    "        processed_snwd_dfs.append(result_df)\n",
    "        print(f\"Number of rows in result_df: {len(result_df)}\")\n",
    "\n",
    "# Concatenate all processed DataFrames into a single DataFrame\n",
    "snwd_df = pd.concat(processed_snwd_dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define processed_dfs for temperature max as an empty list before the loop\n",
    "processed_tmax_dfs = []\n",
    "\n",
    "# Iterate through files in the folder\n",
    "folder_path = \"/mnt/c/Users/tbrag/OneDrive/Desktop/bda602venv/ML Project/weather parquet files/TMAX\"\n",
    "for file_name in os.listdir(folder_path):\n",
    "    if file_name.endswith(\".parquet\"):\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        print(f\"Processing file: {file_path}\")\n",
    "        # Process each file and append the result to processed_dfs\n",
    "        result_df = process_parquet_file(file_path, None, station_list, {\"ID\":\"station_id\",\"DATA_VALUE\":\"TMAX\"})\n",
    "        processed_tmax_dfs.append(result_df)\n",
    "        print(f\"Number of rows in result_df: {len(result_df)}\")\n",
    "\n",
    "# Concatenate all processed DataFrames into a single DataFrame\n",
    "tmax_df = pd.concat(processed_tmax_dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define processed_dfs for temperature min as an empty list before the loop\n",
    "processed_tmin_dfs = []\n",
    "\n",
    "# Iterate through files in the folder\n",
    "folder_path = \"/mnt/c/Users/tbrag/OneDrive/Desktop/bda602venv/ML Project/weather parquet files/TMIN\"\n",
    "for file_name in os.listdir(folder_path):\n",
    "    if file_name.endswith(\".parquet\"):\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        print(f\"Processing file: {file_path}\")\n",
    "        # Process each file and append the result to processed_dfs\n",
    "        result_df = process_parquet_file(file_path, None, station_list, {\"ID\":\"station_id\",\"DATA_VALUE\":\"TMIN\"})\n",
    "        processed_tmin_dfs.append(result_df)\n",
    "        print(f\"Number of rows in result_df: {len(result_df)}\")\n",
    "\n",
    "# Concatenate all processed DataFrames into a single DataFrame\n",
    "tmin_df = pd.concat(processed_tmin_dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Merge tmin_df with tmax_df on station_id and DATE\n",
    "weather_df = pd.merge(tmin_df, tmax_df, on=['station_id', 'DATE'])\n",
    "\n",
    "# Merge merged_df with snow_df on station_id and DATE\n",
    "weather_df = pd.merge(weather_df , snow_df, on=['station_id', 'DATE'])\n",
    "\n",
    "# Merge merged_df with snwd_df on station_id and DATE\n",
    "weather_df = pd.merge(weather_df , snwd_df, on=['station_id', 'DATE'])\n",
    "\n",
    "# Merge merged_df with awnd_df on station_id and DATE\n",
    "weather_df  = pd.merge(weather_df , awnd_df, on=['station_id', 'DATE'])\n",
    "\n",
    "# Merge merged_df with prcp_df on station_id and DATE\n",
    "weather_df = pd.merge(weather_df , prcp_df, on=['station_id', 'DATE'])\n",
    "\n",
    "##export the new table##\n",
    "weather_df.to_csv('weather_data.csv', index=False)\n",
    "weather_df.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'GQW00041415' in station_list:\n",
    "    print(\"yes in the station_list\")\n",
    "else:\n",
    "    print(\"no not in the station_list\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##change date in weather table to datetime object\n",
    "weather_df[\"DATE\"] = pd.to_datetime(weather_df[\"DATE\"], format='%Y%m%d')\n",
    "\n",
    "##export the new table##\n",
    "weather_df.to_csv('weather_data.csv', index=False)\n",
    "\n",
    "len(weather[\"DATE\"].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##change flight date in combined_gdf to date time object\n",
    "combined_gdf[\"FL_DATE\"] = pd.to_datetime(combined_gdf[\"FL_DATE\"])\n",
    "\n",
    "##export the new table##\n",
    "combined_gdf.to_csv('flight_with_station_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "\n",
    "# Convert Pandas DataFrames to Dask DataFrames\n",
    "combined_gdf__dd = dd.from_pandas(combined_gdf, npartitions=4)  \n",
    "weather_df_dd = dd.from_pandas(weather_df, npartitions=4)  \n",
    "\n",
    "# Merge flight_station_data with weather for origin airport\n",
    "flight_weather_data = combined_gdf_dd.merge(weather_df_dd, left_on=['origin_station_id', \"FL_DATE\"],\n",
    "                                           right_on=['station_id', 'DATE'], how='left')\n",
    "\n",
    "# Rename columns to indicate origin airport weather\n",
    "flight_weather_data = flight_weather_data.rename(columns={'AWND': 'ori_AWND', 'PRCP': 'ori_PRCP',\n",
    "                                                          'SNOW': 'ori_SNOW', 'SNWD': 'ori_SNWD',\n",
    "                                                          'TMAX': 'ori_TMAX', 'TMIN': 'ori_TMIN'})\n",
    "\n",
    "# Merge flight_station_data with weather_data for destination weather\n",
    "flight_weather_data = flight_weather_data.merge(weather_df_dd, left_on=['dest_station_id', 'FL_DATE'],\n",
    "                                               right_on=['station_id', 'DATE'], how='left')\n",
    "\n",
    "# Rename columns to indicate destination airport weather\n",
    "flight_weather_data = flight_weather_data.rename(columns={'AWND': 'dest_AWND', 'PRCP': 'dest_PRCP',\n",
    "                                                          'SNOW': 'dest_SNOW', 'SNWD': 'dest_SNWD',\n",
    "                                                          'TMAX': 'dest_TMAX', 'TMIN': 'dest_TMIN'})\n",
    "\n",
    "# Persist the Dask DataFrame if needed for further computations\n",
    "flight_weather_data = flight_weather_data.persist()\n",
    "\n",
    "# Compute the Dask DataFrame to get the actual data (if necessary)\n",
    "flight_weather_data = flight_weather_data.compute()\n",
    "\n",
    "flight_weather_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge flight_data with airport_data for origin airport\n",
    "flight_weather_data = combined_gdf.merge(weather_df, left_on=['origin_station_id',\"FL_DATE\"], right_on=['station_id','DATE'], how='left')\n",
    "\n",
    "# Rename columns to indicate origin airport latitude and longitude\n",
    "flight_weather_data = flight_weather_data.rename(columns={'AWND': 'ori_AWND', 'PRCP': 'ori_PRCP','SNOW':'ori_SNOW','SNWD':'ori_SNWD','TMAX':'ori_TMAX','TMIN':'ori_TMIN'})\n",
    "\n",
    "# Merge flight_data with airport_data for destination airport\n",
    "flight_weather_data = flight_weather_data.merge(weather_df, left_on=['dest_station_id','FL_DATE'], right_on=['station_id','DATE'], how='left')\n",
    "\n",
    "# Rename columns to indicate destination airport latitude and longitude\n",
    "flight_weather_data = flight_weather_data.rename(columns={'AWND': 'dest_AWND', 'PRCP': 'dest_PRCP','SNOW':'dest_SNOW','SNWD':'dest_SNWD','TMAX':'dest_TMAX','TMIN':'dest_TMIN'})\n",
    "\n",
    "# Drop unnecessary columns (iata_x and iata_y)\n",
    "#flight_data.drop(columns=['iata_x', 'iata_y'], inplace=True)\n",
    "\n",
    "flight_weather_data.head(5)\n",
    "\n",
    "##export the new table##\n",
    "#flight_data.to_csv('flight_data_with_coordinates.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bda602",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
